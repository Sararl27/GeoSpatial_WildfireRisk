{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Cloudbutton geospatial use case: 3D fuel mapping for wildfire risk assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "import pylab\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn import model_selection\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.preprocessing import scale, StandardScaler\n",
    "# from sklearn.metrics import silhouette_score\n",
    "# from sklearn.cluster import KMeans\n",
    "# from shapely.geometry import mapping\n",
    "from IPython.display import Image\n",
    "# from IPython.core.display import HTML\n",
    "# from shapely.geometry import mapping\n",
    "# import rasterio as rio\n",
    "# from sklearn.pipeline import Pipeline\n",
    "# from rasterio.mask import mask\n",
    "# from rasterio.plot import show\n",
    "# from rasterio.merge import merge\n",
    "# from rasterio.plot import show\n",
    "# from rasterio.plot import show_hist\n",
    "# from rasterio.windows import Window\n",
    "# from rasterio.plot import reshape_as_raster\n",
    "# from rasterio.plot import reshape_as_image\n",
    "from lithops.storage import Storage\n",
    "import matplotlib.pyplot as plt\n",
    "# import geopandas as gpd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lithops\n",
    "import pathlib\n",
    "# import shapely\n",
    "import shutil\n",
    "# import pickle\n",
    "import subprocess\n",
    "# import concurrent.futures\n",
    "# import joblib\n",
    "# import gdal\n",
    "# import earthpy as ep\n",
    "# import earthpy.spatial as es\n",
    "import time\n",
    "# import glob\n",
    "# import ogr\n",
    "import os\n",
    "# import io\n",
    "# import pdal\n",
    "# import xarray as xr\n",
    "import json\n",
    "# from matplotlib.colors import ListedColormap, BoundaryNorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_BUCKET_TEST = {'optim-size': 'laz_test_size', 'no-partitioned': 'laz_no_particionado', 'well-partitioned': '', 'partitioned':''}\n",
    "DATA_BUCKET_TYPE = {'optim-size':'optim-size','TMP': 'no-partitioned-tmp', 'no-partitioned': 'no-partitioned'} #,'well-partitioned': 'well-partitioned', 'partitioned': 'partitioned'}\n",
    "\n",
    "# LOCAL_INPUT_DIR = fr'E:\\data\\{DATA_BUCKET_TEST[\"no-partitioned\"]}\\20GB\\10GB' # 20GB\\10GB\\2GB\\1GB\\512MB\\256MB\\128MB\n",
    "\n",
    "DATA_BUCKET = f'objects-geospatial-wildfirerisk-{DATA_BUCKET_TYPE[\"optim-size\"]}'\n",
    "INPUT_DATA_PREFIX = f'data-example/' # Modified"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Experiment parameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "FCC_WINDOW = 3\n",
    "FCC_BREAKPOINT = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "Upload dataset\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 09:54:25,594 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n"
     ]
    }
   ],
   "source": [
    "storage = Storage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'bucket_objects = storage.list_keys(bucket=DATA_BUCKET)\\nfor subdir, dirs, files in os.walk(LOCAL_INPUT_DIR):\\n    print(subdir)\\n    for file_name in files:\\n        key = os.path.join(INPUT_DATA_PREFIX, file_name)  # Added\\n        if key not in bucket_objects:   # Changed: if file_name not in bucket_objects:\\n            with open(os.path.join(subdir, file_name), \\'rb\\') as file: #Changed\\n                print(f\\'\\tUploading {key}...\\')\\n                data = file.read()\\n                storage.put_object(bucket=DATA_BUCKET, key=key, body=data)\\n                print(\\'\\tOk!\\')\\n        else:   # Added\\n            print(f\\'\\tIt is already uploaded: {key}...\\')   # Added\\nprint(\"Finished!\")'"
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''bucket_objects = storage.list_keys(bucket=DATA_BUCKET)\n",
    "for subdir, dirs, files in os.walk(LOCAL_INPUT_DIR):\n",
    "    print(subdir)\n",
    "    for file_name in files:\n",
    "        key = os.path.join(INPUT_DATA_PREFIX, file_name)  # Added\n",
    "        if key not in bucket_objects:   # Changed: if file_name not in bucket_objects:\n",
    "            with open(os.path.join(subdir, file_name), 'rb') as file: #Changed\n",
    "                print(f'\\tUploading {key}...')\n",
    "                data = file.read()\n",
    "                storage.put_object(bucket=DATA_BUCKET, key=key, body=data)\n",
    "                print('\\tOk!')\n",
    "        else:   # Added\n",
    "            print(f'\\tIt is already uploaded: {key}...')   # Added\n",
    "print(\"Finished!\")'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "Calculte DEM, DSM and CHM\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_models(obj, storage):\n",
    "    import pdal\n",
    "    from osgeo import gdal\n",
    "    from scipy import ndimage\n",
    "    import time\n",
    "\n",
    "    start_t = time.time()\n",
    "    # Create temporary file paths\n",
    "    tmp_path_prefix = '/tmp/geo/'\n",
    "    if os.path.exists(tmp_path_prefix):\n",
    "        shutil.rmtree(tmp_path_prefix)\n",
    "    for subpath in ['dsm', 'dem', 'chm', 'aspect', 'slope', 'fcc']:\n",
    "        os.makedirs(os.path.join(tmp_path_prefix, subpath), exist_ok=True)\n",
    "\n",
    "    las_tile_filename = pathlib.Path(obj.key).name\n",
    "    tile_key = pathlib.Path(obj.key).stem\n",
    "\n",
    "    # Save obj to file\n",
    "    data = obj.data_stream.read()\n",
    "    input_file_path = os.path.join(tmp_path_prefix, las_tile_filename)\n",
    "    with open(input_file_path, 'wb') as file:\n",
    "        file.write(data)\n",
    "\n",
    "    # DSM pipeline\n",
    "    dsm_file_path = os.path.join(tmp_path_prefix, 'dsm', tile_key + '.gtiff')\n",
    "    dsm_pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": f\"{input_file_path}\",\n",
    "                \"spatialreference\": \"EPSG:25830\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.reprojection\",\n",
    "                \"in_srs\": \"EPSG:25830\",\n",
    "                \"out_srs\": \"EPSG:25830\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.outlier\",\n",
    "                \"method\": \"radius\",\n",
    "                \"radius\": 1.0,\n",
    "                \"min_k\": 4\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.range\",\n",
    "                # Classification equals 2 (corresponding to noise points in LAS).\n",
    "                \"limits\": \"Classification![7:7]\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.range\",\n",
    "                \"limits\": \"returnnumber[1:1]\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.gdal\",\n",
    "                \"gdaldriver\": \"GTiff\",\n",
    "                \"nodata\": \"-9999\",\n",
    "                \"output_type\": \"max\",\n",
    "                \"resolution\": 1,\n",
    "                \"filename\": f\"{dsm_file_path}\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    dsm_pipeline_json_str = json.dumps(dsm_pipeline_json, indent=4)\n",
    "    pipeline = pdal.Pipeline(dsm_pipeline_json_str)\n",
    "    pipeline.validate()\n",
    "    pipeline.loglevel = 8\n",
    "    print('Executing DSM pipeline...')\n",
    "    result = pipeline.execute()\n",
    "    print(result)\n",
    "\n",
    "    # DEM pipeline\n",
    "    dem_file_path = os.path.join(tmp_path_prefix, 'dem', tile_key + '.gtiff')\n",
    "    dem_pipeline_json = {\n",
    "        \"pipeline\": [\n",
    "            {\n",
    "                \"type\": \"readers.las\",\n",
    "                \"filename\": f\"{input_file_path}\",\n",
    "                \"spatialreference\": \"EPSG:25830\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.reprojection\",\n",
    "                \"in_srs\": \"EPSG:25830\",\n",
    "                \"out_srs\": \"EPSG:25830\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.assign\",\n",
    "                \"assignment\": \"Classification[:]=0\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.elm\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.outlier\",\n",
    "                \"method\": \"radius\",\n",
    "                \"radius\": 1.0,\n",
    "                \"min_k\": 4\n",
    "            },\n",
    "            {\n",
    "\n",
    "                \"type\": \"filters.smrf\",\n",
    "                \"ignore\": \"Classification[7:7]\",\n",
    "                \"slope\": 0.2,\n",
    "                \"window\": 16,\n",
    "                \"threshold\": 0.45,\n",
    "                \"scalar\": 1.2\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"filters.range\",\n",
    "                # Classification equals 2 (corresponding to ground in LAS).\n",
    "                \"limits\": \"Classification[2:2]\",\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"writers.gdal\",\n",
    "                \"gdaldriver\": \"GTiff\",\n",
    "                \"nodata\": \"-9999\",\n",
    "                \"output_type\": \"max\",\n",
    "                \"resolution\": 1,\n",
    "                \"filename\": f\"{dem_file_path}\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    dem_pipeline_json_str = json.dumps(dem_pipeline_json, indent=4)\n",
    "    pipeline = pdal.Pipeline(dem_pipeline_json_str)\n",
    "    pipeline.validate()  # Check if json options are good\n",
    "    pipeline.loglevel = 8\n",
    "    print('Executing DEM pipeline...')\n",
    "    result = pipeline.execute()\n",
    "    print(result)\n",
    "\n",
    "    # calculate CHM\n",
    "    chm_file_path = os.path.join(tmp_path_prefix, 'chm', tile_key + '.tiff')\n",
    "    cmd = ['gdal_calc.py', '-A', dem_file_path, '-B', dsm_file_path,\n",
    "           '--calc=\"B-A\"', '--NoDataValue=0', '--outfile', chm_file_path]\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n",
    "    stdout, stderr = p.communicate()\n",
    "    print(stdout, stderr)\n",
    "    # assert p.returncode == 0\n",
    "\n",
    "    # calculate aspect\n",
    "    aspect_file_path = os.path.join(tmp_path_prefix, 'aspect', tile_key + '.tiff')\n",
    "    cmd = ['gdaldem', 'aspect', dem_file_path, aspect_file_path, '-compute_edges']\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n",
    "    stdout, stderr = p.communicate()\n",
    "    print(stdout, stderr)\n",
    "    # assert p.returncode == 0\n",
    "\n",
    "    # calculate slope\n",
    "    slope_file_path = os.path.join(tmp_path_prefix, 'slope', tile_key + '.tiff')\n",
    "    cmd = ['gdaldem', 'slope', dem_file_path, slope_file_path, '-compute_edges']\n",
    "    p = subprocess.Popen(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, close_fds=True)\n",
    "    stdout, stderr = p.communicate()\n",
    "    print(stdout, stderr)\n",
    "    # assert p.returncode == 0\n",
    "\n",
    "    # calculate FCC\n",
    "    in_ds = gdal.Open(dem_file_path)\n",
    "    rows = in_ds.RasterYSize\n",
    "    cols = in_ds.RasterXSize\n",
    "    in_band = in_ds.GetRasterBand(1)\n",
    "    data = in_band.ReadAsArray(0, 0, cols, rows).astype(np.float)\n",
    "    data[data > FCC_BREAKPOINT] = 1\n",
    "    data[data <= FCC_BREAKPOINT] = 0\n",
    "\n",
    "    # Computing fraction on the whole raster through a moving window.\n",
    "    def _compute_fraction(array):\n",
    "        nveg = np.sum(array == 1)\n",
    "        total = len(array)\n",
    "        out = (nveg/total)*100\n",
    "        return(out)\n",
    "\n",
    "    TCC = ndimage.generic_filter(data, _compute_fraction, size=FCC_WINDOW)\n",
    "\n",
    "    gtiff_driver = gdal.GetDriverByName(\"GTiff\")\n",
    "    fcc_file_path = os.path.join(tmp_path_prefix, 'fcc', tile_key + '.tiff')\n",
    "    out_ds = gtiff_driver.Create(fcc_file_path, cols, rows, 1, in_band.DataType)\n",
    "    out_ds.SetProjection(in_ds.GetProjection())\n",
    "    out_ds.SetGeoTransform(in_ds.GetGeoTransform())\n",
    "\n",
    "    out_band = out_ds.GetRasterBand(1)\n",
    "    out_band.WriteArray(TCC)\n",
    "    # out_ds.BuildOverviews(\"Average\", [2, 4, 8, 16, 32])\n",
    "    out_ds.FlushCache()\n",
    "    del in_ds, out_ds\n",
    "\n",
    "    outputs = [dsm_file_path, dem_file_path, chm_file_path,\n",
    "               aspect_file_path, slope_file_path, fcc_file_path]\n",
    "    for output_path in outputs:\n",
    "        if os.path.exists(output_path):\n",
    "            with open(output_path, 'rb') as output_file:\n",
    "                data = output_file.read()\n",
    "                cos_key = output_path.replace(tmp_path_prefix, '')\n",
    "                storage.put_object(bucket=DATA_BUCKET, key=cos_key, body=data)\n",
    "        else:\n",
    "            print(f'Failed to upload {output_path}')\n",
    "\n",
    "    # out = subprocess.check_output(['find', '/tmp/geo/'])\n",
    "    return f'Time: {time.time() - start_t} seconds -> Key: {obj.key}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 09:54:25,777 [INFO] lithops.config -- Lithops v2.6.0\n",
      "2022-06-17 09:54:25,804 [INFO] lithops.storage.backends.ibm_cos.ibm_cos -- IBM COS client created - Region: eu-de\n",
      "2022-06-17 09:54:25,808 [INFO] lithops.serverless.backends.ibm_cf.ibm_cf -- IBM CF client created - Region: eu-de - Namespace: my-geospatial-wildfirerisk_foundry-geospatial-wildfirerisk-1\n"
     ]
    }
   ],
   "source": [
    "fexec = lithops.FunctionExecutor(monitoring='rabbitmq')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 09:54:25,836 [INFO] lithops.invokers -- ExecutorID 6b255e-6 | JobID M000 - Selected Runtime: sararl27/lithops-ibm_cf-runtime-3.9:0.1 - 2048MB\n",
      "2022-06-17 09:54:26,558 [INFO] lithops.invokers -- ExecutorID 6b255e-6 | JobID M000 - Starting function invocation: calculate_models() - Total: 49 activations\n",
      "2022-06-17 09:54:26,692 [INFO] lithops.invokers -- ExecutorID 6b255e-6 | JobID M000 - View execution logs at C:\\Users\\saral\\AppData\\Local\\Temp\\lithops\\logs\\6b255e-6-M000.log\n"
     ]
    },
    {
     "data": {
      "text/plain": "[<lithops.future.ResponseFuture at 0x289c9bace80>,\n <lithops.future.ResponseFuture at 0x289c9bac3d0>,\n <lithops.future.ResponseFuture at 0x289c9c121f0>,\n <lithops.future.ResponseFuture at 0x289c9c20970>,\n <lithops.future.ResponseFuture at 0x289c9c20880>,\n <lithops.future.ResponseFuture at 0x289c9c20910>,\n <lithops.future.ResponseFuture at 0x289c9c20940>,\n <lithops.future.ResponseFuture at 0x289c9c20a90>,\n <lithops.future.ResponseFuture at 0x289c9c20ac0>,\n <lithops.future.ResponseFuture at 0x289c9c20af0>,\n <lithops.future.ResponseFuture at 0x289c9c20b20>,\n <lithops.future.ResponseFuture at 0x289c9c20b50>,\n <lithops.future.ResponseFuture at 0x289c9c20b80>,\n <lithops.future.ResponseFuture at 0x289c9c20bb0>,\n <lithops.future.ResponseFuture at 0x289c9c20be0>,\n <lithops.future.ResponseFuture at 0x289c9c20c10>,\n <lithops.future.ResponseFuture at 0x289c9c20c40>,\n <lithops.future.ResponseFuture at 0x289c9c20c70>,\n <lithops.future.ResponseFuture at 0x289c9c20ca0>,\n <lithops.future.ResponseFuture at 0x289c9c20cd0>,\n <lithops.future.ResponseFuture at 0x289c9c20d00>,\n <lithops.future.ResponseFuture at 0x289c9c20d30>,\n <lithops.future.ResponseFuture at 0x289c9c20d60>,\n <lithops.future.ResponseFuture at 0x289c9c20d90>,\n <lithops.future.ResponseFuture at 0x289c9c20dc0>,\n <lithops.future.ResponseFuture at 0x289c9c20df0>,\n <lithops.future.ResponseFuture at 0x289c9c20e20>,\n <lithops.future.ResponseFuture at 0x289c9c20e50>,\n <lithops.future.ResponseFuture at 0x289c9c20e80>,\n <lithops.future.ResponseFuture at 0x289c9c20eb0>,\n <lithops.future.ResponseFuture at 0x289c9c20ee0>,\n <lithops.future.ResponseFuture at 0x289c9c20f10>,\n <lithops.future.ResponseFuture at 0x289c9c20f40>,\n <lithops.future.ResponseFuture at 0x289c9c20f70>,\n <lithops.future.ResponseFuture at 0x289c9c20fa0>,\n <lithops.future.ResponseFuture at 0x289c9c20fd0>,\n <lithops.future.ResponseFuture at 0x289c9c30040>,\n <lithops.future.ResponseFuture at 0x289c9c30070>,\n <lithops.future.ResponseFuture at 0x289c9c300a0>,\n <lithops.future.ResponseFuture at 0x289c9c300d0>,\n <lithops.future.ResponseFuture at 0x289c9c30100>,\n <lithops.future.ResponseFuture at 0x289c9c30130>,\n <lithops.future.ResponseFuture at 0x289c9c30160>,\n <lithops.future.ResponseFuture at 0x289c9c30190>,\n <lithops.future.ResponseFuture at 0x289c9c301c0>,\n <lithops.future.ResponseFuture at 0x289c9c301f0>,\n <lithops.future.ResponseFuture at 0x289c9c30220>,\n <lithops.future.ResponseFuture at 0x289c9c30250>,\n <lithops.future.ResponseFuture at 0x289c9c30280>]"
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fexec.map(calculate_models, f'cos://{DATA_BUCKET}/{INPUT_DATA_PREFIX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 09:54:27,314 [INFO] lithops.wait -- ExecutorID 6b255e-6 - Getting results from 49 function activations\n"
     ]
    },
    {
     "data": {
      "text/plain": "    0%|          | 0/49  ",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "255f2cb1e9dc414b9bc5fcab86a693e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-17 10:01:24,047 [INFO] lithops.executors -- ExecutorID 6b255e-6 - Cleaning temporary data\n"
     ]
    }
   ],
   "source": [
    "res = fexec.get_result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 149.46400380134583 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (10) - copia.laz\n",
      "---\n",
      "Time: 122.35160899162292 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (10).laz\n",
      "---\n",
      "Time: 151.81607031822205 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (11) - copia.laz\n",
      "---\n",
      "Time: 186.48723888397217 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (11).laz\n",
      "---\n",
      "Time: 112.28131794929504 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (12) - copia.laz\n",
      "---\n",
      "Time: 159.0072054862976 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (12).laz\n",
      "---\n",
      "Time: 93.48393249511719 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (13) - copia.laz\n",
      "---\n",
      "Time: 156.4616961479187 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (13).laz\n",
      "---\n",
      "Time: 182.0946855545044 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (14) - copia.laz\n",
      "---\n",
      "Time: 106.26676487922668 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (14).laz\n",
      "---\n",
      "Time: 126.5676577091217 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (15) - copia.laz\n",
      "---\n",
      "Time: 119.44001173973083 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (16) - copia.laz\n",
      "---\n",
      "Time: 116.05271816253662 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (17) - copia.laz\n",
      "---\n",
      "Time: 125.34732174873352 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (18) - copia.laz\n",
      "---\n",
      "Time: 157.27930068969727 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (19) - copia.laz\n",
      "---\n",
      "Time: 151.21580576896667 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (2) - copia.laz\n",
      "---\n",
      "Time: 108.97966003417969 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (2).laz\n",
      "---\n",
      "Time: 106.8440158367157 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (20) - copia - copia.laz\n",
      "---\n",
      "Time: 105.9258165359497 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (20) - copia.laz\n",
      "---\n",
      "Time: 188.20785403251648 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (21) - copia - copia.laz\n",
      "---\n",
      "Time: 115.02607035636902 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (21) - copia.laz\n",
      "---\n",
      "Time: 121.0392894744873 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (22) - copia - copia.laz\n",
      "---\n",
      "Time: 103.25102496147156 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (22) - copia.laz\n",
      "---\n",
      "Time: 123.65219187736511 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (23) - copia - copia.laz\n",
      "---\n",
      "Time: 105.06617474555969 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (23) - copia.laz\n",
      "---\n",
      "Time: 104.71153473854065 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (24) - copia - copia.laz\n",
      "---\n",
      "Time: 126.39355731010437 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (24) - copia.laz\n",
      "---\n",
      "Time: 153.71116876602173 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (25) - copia - copia.laz\n",
      "---\n",
      "Time: 119.19767498970032 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (25) - copia.laz\n",
      "---\n",
      "Time: 105.95326209068298 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (3) - copia.laz\n",
      "---\n",
      "Time: 105.30560684204102 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (3).laz\n",
      "---\n",
      "Time: 187.31700992584229 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (4) - copia.laz\n",
      "---\n",
      "Time: 123.29942727088928 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (4).laz\n",
      "---\n",
      "Time: 123.22706866264343 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (5) - copia.laz\n",
      "---\n",
      "Time: 101.93016815185547 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (5).laz\n",
      "---\n",
      "Time: 179.95099115371704 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (6) - copia.laz\n",
      "---\n",
      "Time: 158.90307879447937 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (6).laz\n",
      "---\n",
      "Time: 90.6273832321167 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (7) - copia.laz\n",
      "---\n",
      "Time: 408.1469249725342 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (7).laz\n",
      "---\n",
      "Time: 103.21329975128174 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (8) - copia.laz\n",
      "---\n",
      "Time: 180.90378999710083 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (8).laz\n",
      "---\n",
      "Time: 124.19916319847107 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (9) - copia.laz\n",
      "---\n",
      "Time: 122.77850675582886 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (9).laz\n",
      "---\n",
      "Time: 188.69640803337097 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia - copia (3).laz\n",
      "---\n",
      "Time: 109.55019068717957 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia - copia (4).laz\n",
      "---\n",
      "Time: 108.13173484802246 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia - copia (5).laz\n",
      "---\n",
      "Time: 121.30960655212402 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia - copia.laz\n",
      "---\n",
      "Time: 403.84403467178345 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia.laz\n",
      "---\n",
      "Time: 124.87991881370544 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB.laz\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "max_t = [0,'']\n",
    "for r in res:\n",
    "    tmp = float(r.split(' ')[1])\n",
    "    if tmp > max_t[0]:\n",
    "        max_t[0] = tmp\n",
    "        max_t[1] = r\n",
    "    print(r)\n",
    "    print('---')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max = Time: 408.1469249725342 seconds -> Key: data-example/PNOA_2017_CLM-CAS_278-4470_ORT-CLA-RGB - copia (7).laz\n"
     ]
    }
   ],
   "source": [
    "print(f'Max = {max_t[1]}')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "Scalability\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "def get_process_cost(fexec):\n",
    "    import pandas as pd\n",
    "    df = pd.read_csv(fexec.log_path)\n",
    "    cost = float(df[df[\"Job_ID\"] == \"Summary\"][\"Cost\"])\n",
    "    return cost"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Time “calculate_models”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "# Statistics:\n",
    "endTime = set()\n",
    "startTime = set()\n",
    "\n",
    "for future in fexec.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"worker_func_start_tstamp\"):\n",
    "            startTime.add(future.stats[key])\n",
    "        if key.endswith(\"worker_func_end_tstamp\"):\n",
    "            endTime.add(future.stats[key])\n",
    "\n",
    "timeExec = max(endTime) - min(startTime)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 414.03885197639465 seconds\n"
     ]
    }
   ],
   "source": [
    "print(f\"Time: {timeExec} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "fexec.plot(dst=\"plots/scalability_wildFireRisk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/scalability_wildFireRisk_histogram.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Image(filename=\"plots/scalability_wildFireRisk_timeline.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cost \"calculate_models\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fexec.job_summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.read_csv(fexec.log_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cost_interpolation = get_process_cost(fexec)\n",
    "print(f\"The experiment cost of 'calculate_models' ${cost_interpolation:.4f}.\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---\n",
    "Throughput\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fexec = lithops.FunctionExecutor()\n",
    "storage = Storage()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def storage_performance_read(key_name, storage):\n",
    "    get_t0 = time.time()\n",
    "    storage.get_object(bucket=DATA_BUCKET, key=key_name)\n",
    "    get_t1 = time.time()\n",
    "    get_sz = len(key_name)\n",
    "    read_bandwidth_mb = get_sz / (get_t1 - get_t0) / 1e6\n",
    "\n",
    "    stats = {'t0': get_t0, 't1': get_t1, 'bandwidth': read_bandwidth_mb, 'size': get_sz}\n",
    "\n",
    "    return stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data_file_read = [str(obj['Key']) for obj in storage.list_objects(DATA_BUCKET) if str(obj['Key']).endswith('.laz') or str(obj['Key']).endswith('.las')]\n",
    "\n",
    "fexec = lithops.FunctionExecutor()\n",
    "fs = fexec.map(storage_performance_read, data_file_read)\n",
    "read_results = fexec.get_result(fs=fs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Write"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def storage_performance_write(obj, size_file, storage):\n",
    "    data = obj.data_stream.read()\n",
    "    put_sz = size_file # Modified, get the size in bytes of the data to upload\n",
    "    put_t0 = time.time()\n",
    "    storage.put_object(bucket=DATA_BUCKET,  key=obj.key, body=data)\n",
    "    put_t1 = time.time()\n",
    "    write_bandwidth_mb = put_sz / (put_t1 - put_t0) / 1e6\n",
    "\n",
    "    stats = {'t0': put_t0, 't1': put_t1, 'bandwidth': write_bandwidth_mb, 'size': put_sz}\n",
    "\n",
    "    return stats"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data_file_write = [{'obj':(f'cos://{DATA_BUCKET}/'+obj[\"Key\"]), 'size_file': obj['Size']} for obj in storage.list_objects(DATA_BUCKET) if str(obj['Key']).endswith('.tiff') or str(obj['Key']).endswith('.gtiff')]\n",
    "fexec = lithops.FunctionExecutor()\n",
    "fs = fexec.map(storage_performance_write, data_file_write)\n",
    "write_results = fexec.get_result(fs=fs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Resume"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def create_agg_bdwth_plot(res_write, res_read, dst):\n",
    "    def compute_times_rates(start_time, d):\n",
    "        x = np.array(d)\n",
    "        tzero = start_time\n",
    "        tr_start_time = x[:, 0] - tzero\n",
    "        tr_end_time = x[:, 1] - tzero\n",
    "        rate = x[:, 2]\n",
    "\n",
    "        N = len(tr_start_time)\n",
    "        runtime_rate_hist = np.zeros((N, len(runtime_bins)))\n",
    "\n",
    "        for i in range(N):\n",
    "            s = tr_start_time[i]\n",
    "            e = tr_end_time[i]\n",
    "            a, b = np.searchsorted(runtime_bins, [s, e])\n",
    "            if b-a > 0:\n",
    "                runtime_rate_hist[i, a:b] = rate[i]\n",
    "\n",
    "        return {'start_time': tr_start_time,\n",
    "                'end_time': tr_end_time,\n",
    "                'rate': rate,\n",
    "                'runtime_rate_hist': runtime_rate_hist}\n",
    "\n",
    "    start_time = min((min(t['t0'] for t in res_write), (min(t['t0'] for t in res_read)))) - 1\n",
    "\n",
    "    fig = pylab.figure(figsize=(8, 6))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    for datum, l in [(res_write, 'Aggregate Write Bandwidth'), (res_read, 'Aggregate Read Bandwidth')]:\n",
    "        mb_rates = [(res['t0'], res['t1'], res['bandwidth']) for res in datum]\n",
    "        max_seconds = int(max([mr[1]-start_time for mr in mb_rates])*1.2)\n",
    "        max_seconds = 8 * round(max_seconds/8)\n",
    "        runtime_bins = np.linspace(0, max_seconds, max_seconds)\n",
    "\n",
    "        mb_rates_hist = compute_times_rates(start_time, mb_rates)\n",
    "\n",
    "        ax.plot(mb_rates_hist['runtime_rate_hist'].sum(axis=0)/1000, label=l)\n",
    "\n",
    "    ax.set_xlabel('Execution Time (sec)')\n",
    "    ax.set_ylabel(\"GB/sec\")\n",
    "    ax.set_xlim(0, )\n",
    "    ax.set_ylim(0, )\n",
    "    pylab.legend()\n",
    "    pylab.grid(True, axis='y')\n",
    "\n",
    "    dst = os.path.expanduser(dst) if '~' in dst else dst\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(dst, format='png')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_agg_bdwth_plot(write_results, read_results, './plots/throughput_storage_kpi.png')\n",
    "Image(filename=\"plots/throughput_storage_kpi.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calc_mean_throughput(dict):\n",
    "    res = 0\n",
    "    for val in dict:\n",
    "        res += val['bandwidth']\n",
    "    return (res/len(dict))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f'Mean throughput read: {calc_mean_throughput(read_results)} MB/s')\n",
    "print(f'Mean throughput write: {calc_mean_throughput(write_results)} MB/s')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Time \"throughput\""
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Statistics:\n",
    "endTime = set()\n",
    "startTime = set()\n",
    "\n",
    "for future in fexec.futures:\n",
    "    for key in future.stats.keys():\n",
    "        if key.endswith(\"worker_func_start_tstamp\"):\n",
    "            startTime.add(future.stats[key])\n",
    "        if key.endswith(\"worker_func_end_tstamp\"):\n",
    "            endTime.add(future.stats[key])\n",
    "\n",
    "timeExec = max(endTime) - min(startTime)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "print(f\"Time: {timeExec} seconds\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "fexec.plot(dst=\"plots/scalability_throughput\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename=\"plots/scalability_throughput_histogram.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "Image(filename=\"plots/scalability_throughput_timeline.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Cost throughput"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fexec.job_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pd.read_csv(fexec.log_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cost_interpolation = get_process_cost(fexec)\n",
    "print(f\"The experiment cost of 'throughput' ${cost_interpolation:.4f}.\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fc28e218f8aa598d7201f1426aba0b64c29ddf364b0850b5108205e3c665e009"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}